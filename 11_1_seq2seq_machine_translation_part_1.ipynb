{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abdalrahmenyousifMohamed/Master-TensorFlow/blob/main/11_1_seq2seq_machine_translation_part_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBPUJbY91kHK"
      },
      "source": [
        "# Seq2Seq models (Sequence-to-Sequence)\n",
        "\n",
        "Sequence to sequence models are a variant of deep learning models that consists of an encoder and a decoder. They are used for problems that map an abitrarily long sequence to another arbitrarliy long sequence. For example, in machine translation, you convert a sequence of words in a source language to a sequence of words in a target language. Here we will see how we can use a seq2seq model to solve a machine translation task to convert English to German.\n",
        "\n",
        "\n",
        "<table align=\"left\">\n",
        "    <td>\n",
        "        <a target=\"_blank\" href=\"https://colab.research.google.com/github/thushv89/manning_tf2_in_action/blob/master/Ch11-Ch12-Sequence-to-Sequence-Learning-with-TF2/11.1_seq2seq_machine_translation_part_1.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "    </td>\n",
        "</table>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwKuKrPo1kHN",
        "outputId": "7dc8aed9-f1e1-4210-9b83-d5475585578e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.14.0\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time\n",
        "import json\n",
        "\n",
        "def fix_random_seed(seed):\n",
        "    \"\"\" Setting the random seed of various libraries \"\"\"\n",
        "    try:\n",
        "        np.random.seed(seed)\n",
        "    except NameError:\n",
        "        print(\"Warning: Numpy is not imported. Setting the seed for Numpy failed.\")\n",
        "    try:\n",
        "        tf.random.set_seed(seed)\n",
        "    except NameError:\n",
        "        print(\"Warning: TensorFlow is not imported. Setting the seed for TensorFlow failed.\")\n",
        "    try:\n",
        "        random.seed(seed)\n",
        "    except NameError:\n",
        "        print(\"Warning: random module is not imported. Setting the seed for random failed.\")\n",
        "\n",
        "# Fixing the random seed\n",
        "random_seed=4321\n",
        "fix_random_seed(random_seed)\n",
        "\n",
        "print(\"TensorFlow version: {}\".format(tf.__version__))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tL3IrWXP1kHO"
      },
      "source": [
        "http://www.manythings.org/anki/\n",
        "    \n",
        "german-english"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvfxi7wg1kHP",
        "outputId": "83446b6e-6701-4d1d-938b-11db1df128de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: TF_FORCE_GPU_ALLOW_GROWTH=true\n"
          ]
        }
      ],
      "source": [
        "# Not setting this led to the following error\n",
        "# _Derived_]RecvAsync is cancelled.\n",
        "# [[{{node gradient_tape/model_1/embedding_1/embedding_lookup/Reshape/_172}}]] [Op:__inference_train_function_31985]\n",
        "\n",
        "%env TF_FORCE_GPU_ALLOW_GROWTH=true"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrKzKg8p1kHP"
      },
      "source": [
        "## Loading the data (Requires manual download)\n",
        "\n",
        "Unfortunately, this dataset **must be manually downloaded** by clicking [this link](http://www.manythings.org/anki/deu-eng.zip). Then place the downloaded `deu-eng.zip` file in the `Ch11/data` folder before running the cell below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6p_MCYXf1kHQ",
        "outputId": "d10cf96f-91ec-487b-f971-081e2db5cb8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The extracted data already exists\n"
          ]
        }
      ],
      "source": [
        "# Section 11.1\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "\n",
        "# Make sure the zip file has been downloaded\n",
        "# if not os.path.exists(os.path.join('data','deu-eng.zip')):\n",
        "#     raise FileNotFoundError(\n",
        "#         \"Uh oh! Did you download the deu-eng.zip from http://www.manythings.org/anki/deu-eng.zip manually and place it in the Ch11/data folder?\"\n",
        "#     )\n",
        "\n",
        "# else:\n",
        "if not os.path.exists(os.path.join('data', 'deu.txt')):\n",
        "        with zipfile.ZipFile(os.path.join('data','deu-eng.zip'), 'r') as zip_ref:\n",
        "            zip_ref.extractall('data')\n",
        "else:\n",
        "        print(\"The extracted data already exists\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKnWRypp1kHQ"
      },
      "source": [
        "## Reading the data\n",
        "\n",
        "Data is in a single `.txt` file. It is a parallel corpus meaning there is a English sentence/phrase/paragraph and a corresponding German translation of it side-by-side. In the file, the source input and the translation are separated by a tab (i.e. tab-seperated file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdIvMjUG1kHR",
        "outputId": "5de7774c-56cb-4d03-f657-44cdf0924aa0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "df.shape = (176692, 2)\n"
          ]
        }
      ],
      "source": [
        "# Section 11.1\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Read the csv file\n",
        "df = pd.read_csv(os.path.join('data', 'deu.txt'), delimiter='\\t', encoding='utf-8', encoding_errors=\"strict\", header=None)\n",
        "# Set column names\n",
        "df.columns = [\"EN\", \"DE\"]\n",
        "df = df[[\"EN\", \"DE\"]]\n",
        "print('df.shape = {}'.format(df.shape))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "c6lfy7tB6E0R",
        "outputId": "dc1b8158-d719-40fd-8c27-33ce29f0a94c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     EN             DE\n",
              "0   Hi.         Hallo!\n",
              "1   Hi.     Grüß Gott!\n",
              "2  Run!          Lauf!\n",
              "3  Wow!    Potzdonner!\n",
              "4  Wow!  Donnerwetter!"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e40272be-2a93-4f37-ac35-cf8a3dec7538\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>EN</th>\n",
              "      <th>DE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Hi.</td>\n",
              "      <td>Hallo!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Hi.</td>\n",
              "      <td>Grüß Gott!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Run!</td>\n",
              "      <td>Lauf!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Wow!</td>\n",
              "      <td>Potzdonner!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Wow!</td>\n",
              "      <td>Donnerwetter!</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e40272be-2a93-4f37-ac35-cf8a3dec7538')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e40272be-2a93-4f37-ac35-cf8a3dec7538 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e40272be-2a93-4f37-ac35-cf8a3dec7538');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-72340df2-b35e-47ba-99c1-98f71e905734\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-72340df2-b35e-47ba-99c1-98f71e905734')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-72340df2-b35e-47ba-99c1-98f71e905734 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "KbTBocsj1kHR"
      },
      "outputs": [],
      "source": [
        "# There are \\xc2\\xa0 (undecode-able bytes remaining in some text)\n",
        "# This can cause errors like UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc2 in position 3: unexpected end of data\n",
        "# when using the TextVectorization layer\n",
        "clean_inds = [i for i in range(len(df)) if b\"\\xc2\" not in df.iloc[i][\"DE\"].encode(\"utf-8\")]\n",
        "\n",
        "df = df.iloc[clean_inds]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_inds"
      ],
      "metadata": {
        "id": "iAFuLSyb6bjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "PAFj2Zpx1kHR",
        "outputId": "da463cc3-53f9-4d3b-9b9e-e89df4328245"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     EN             DE\n",
              "0   Hi.         Hallo!\n",
              "1   Hi.     Grüß Gott!\n",
              "2  Run!          Lauf!\n",
              "3  Wow!    Potzdonner!\n",
              "4  Wow!  Donnerwetter!"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a7dd5c8e-c099-4d65-a9d4-1ed17b631342\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>EN</th>\n",
              "      <th>DE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Hi.</td>\n",
              "      <td>Hallo!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Hi.</td>\n",
              "      <td>Grüß Gott!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Run!</td>\n",
              "      <td>Lauf!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Wow!</td>\n",
              "      <td>Potzdonner!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Wow!</td>\n",
              "      <td>Donnerwetter!</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a7dd5c8e-c099-4d65-a9d4-1ed17b631342')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a7dd5c8e-c099-4d65-a9d4-1ed17b631342 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a7dd5c8e-c099-4d65-a9d4-1ed17b631342');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-28648ee4-2a55-440f-b31f-6ed46d0d8df8\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-28648ee4-2a55-440f-b31f-6ed46d0d8df8')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-28648ee4-2a55-440f-b31f-6ed46d0d8df8 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "U2O_GiV11kHS",
        "outputId": "94748b96-3784-445e-e933-7b97fc6674c4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                       EN  \\\n",
              "176686  We recommend adding sentences and translations...   \n",
              "176687  Death is something that we're often discourage...   \n",
              "176688  At a moment when our economy is growing, our b...   \n",
              "176689  If someone who doesn't know your background sa...   \n",
              "176690  If someone who doesn't know your background sa...   \n",
              "\n",
              "                                                       DE  \n",
              "176686  Wir empfehlen, nur in der am besten beherrscht...  \n",
              "176687  Wir werden oft davon abgehalten, über den Tod ...  \n",
              "176688  In einem Moment, in dem unsere Wirtschaft wäch...  \n",
              "176689  Wenn jemand, der deine Herkunft nicht kennt, s...  \n",
              "176690  Wenn jemand Fremdes dir sagt, dass du dich wie...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-df48b97f-2418-4e11-9431-8cef08c4addf\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>EN</th>\n",
              "      <th>DE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>176686</th>\n",
              "      <td>We recommend adding sentences and translations...</td>\n",
              "      <td>Wir empfehlen, nur in der am besten beherrscht...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176687</th>\n",
              "      <td>Death is something that we're often discourage...</td>\n",
              "      <td>Wir werden oft davon abgehalten, über den Tod ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176688</th>\n",
              "      <td>At a moment when our economy is growing, our b...</td>\n",
              "      <td>In einem Moment, in dem unsere Wirtschaft wäch...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176689</th>\n",
              "      <td>If someone who doesn't know your background sa...</td>\n",
              "      <td>Wenn jemand, der deine Herkunft nicht kennt, s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176690</th>\n",
              "      <td>If someone who doesn't know your background sa...</td>\n",
              "      <td>Wenn jemand Fremdes dir sagt, dass du dich wie...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-df48b97f-2418-4e11-9431-8cef08c4addf')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-df48b97f-2418-4e11-9431-8cef08c4addf button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-df48b97f-2418-4e11-9431-8cef08c4addf');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-82d7fb72-91f0-405e-a530-107f01858cfd\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-82d7fb72-91f0-405e-a530-107f01858cfd')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-82d7fb72-91f0-405e-a530-107f01858cfd button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "df.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtwwHEbR1kHS"
      },
      "source": [
        "## Use a smaller sample for computational speed\n",
        "\n",
        "There are more than 220000 samples in the original dataset. We will be using a smaller set of 50000 for our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "dydyfhgn1kHS"
      },
      "outputs": [],
      "source": [
        "n_samples = 50000\n",
        "df = df.sample(n=n_samples, random_state=random_seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7NQ18l91kHT"
      },
      "source": [
        "## Introducing the `SOS` and `EOS` tokens (Decoder)\n",
        "\n",
        "We will add these special tokens to the translated targets. `sos` indicates the start of the sentence and `eos` marks the end of the sentence.\n",
        "\n",
        "E.g. `Grüß Gott!` becomes `sos Grüß Gott! eos`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "s42YMN2z1kHT"
      },
      "outputs": [],
      "source": [
        "start_token = 'sos'\n",
        "end_token = 'eos'\n",
        "\n",
        "df[\"DE\"] = start_token + ' ' + df[\"DE\"] + ' ' + end_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRKAIxwH1kHU"
      },
      "source": [
        "## Splitting training/validation/testing data\n",
        "\n",
        "We will be creating three datasets by sampling randomly (without replacement);\n",
        "\n",
        "* Test dataset - 5000 samples\n",
        "* Validation dataset - 5000 samples\n",
        "* Training dataset - 40000 samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9jzlYos1kHU",
        "outputId": "906b15b5-8a95-4df1-e72e-cf8fc7b55747"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_df.shape = (5000, 2)\n",
            "valid_df.shape = (5000, 2)\n",
            "train_df.shape = (40000, 2)\n"
          ]
        }
      ],
      "source": [
        "# Randomly sample 5000 examples from the total 50000 randomly\n",
        "test_df = df.sample(n=int(n_samples/10), random_state=random_seed)\n",
        "# Randomly sample 5000 examples from the total 50000 randomly\n",
        "valid_df = df.loc[~df.index.isin(test_df.index)].sample(n=int(n_samples/10), random_state=random_seed)\n",
        "# Assign the rest to training data\n",
        "train_df = df.loc[~(df.index.isin(test_df.index) | df.index.isin(valid_df.index))]\n",
        "\n",
        "print('test_df.shape = {}'.format(test_df.shape))\n",
        "print('valid_df.shape = {}'.format(valid_df.shape))\n",
        "print('train_df.shape = {}'.format(train_df.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqltcCq71kHU"
      },
      "source": [
        "## Analysing the vocabulary sizes (English and German)\n",
        "\n",
        "Calculate the vocabulary size. We will only consider the words that appear at least 10 times in the corpus."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df[\"EN\"].tolist()"
      ],
      "metadata": {
        "id": "DX6Du3Dh9eF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Atf-Oebd1kHV",
        "outputId": "089810af-762c-4e2f-8708-6ff6b10333d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English corpus\n",
            "==================================================\n",
            "to     9146\n",
            "I      8600\n",
            "Tom    8369\n",
            "the    7143\n",
            "you    6242\n",
            "a      5675\n",
            "is     4366\n",
            "in     2898\n",
            "of     2685\n",
            "was    2339\n",
            "dtype: int64\n",
            "\n",
            "Vocabulary size (>=10 frequent): 2249\n",
            "\n",
            "German corpus\n",
            "==================================================\n",
            "sos      40000\n",
            "eos      40000\n",
            "Tom       8921\n",
            "Ich       7774\n",
            "ist       4702\n",
            "nicht     4402\n",
            "zu        3766\n",
            "Sie       3735\n",
            "du        3183\n",
            "das       2782\n",
            "dtype: int64\n",
            "\n",
            "Vocabulary size (>=10 frequent): 2531\n"
          ]
        }
      ],
      "source": [
        "# Section 11.1\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "# Create a flattened list from English words\n",
        "en_words = train_df[\"EN\"].str.split().sum()\n",
        "# Create a flattened list of German words\n",
        "de_words = train_df[\"DE\"].str.split().sum()\n",
        "\n",
        "# Get the vocabulary size of words appearing more than or equal to 10 times\n",
        "n=10\n",
        "\n",
        "# Code listing 11.1\n",
        "def get_vocabulary_size_greater_than(words, n, verbose=True):\n",
        "\n",
        "    \"\"\" Get the vocabulary size above a certain threshold \"\"\"\n",
        "\n",
        "    # Generate a counter object i.e. dict word -> frequency\n",
        "    counter = Counter(words)\n",
        "\n",
        "    # Create a pandas series from the counter, then sort most frequent to least\n",
        "    freq_df = pd.Series(list(counter.values()), index=list(counter.keys())).sort_values(ascending=False)\n",
        "\n",
        "    if verbose:\n",
        "        # Print most common words\n",
        "        print(freq_df.head(n=10))\n",
        "\n",
        "    # Count of words >= n frequent\n",
        "    n_vocab = (freq_df>=n).sum()\n",
        "\n",
        "    if verbose:\n",
        "        print(\"\\nVocabulary size (>={} frequent): {}\".format(n, n_vocab))\n",
        "\n",
        "    return n_vocab\n",
        "\n",
        "print(\"English corpus\")\n",
        "print('='*50)\n",
        "en_vocab = get_vocabulary_size_greater_than(en_words, n)\n",
        "\n",
        "print(\"\\nGerman corpus\")\n",
        "print('='*50)\n",
        "de_vocab = get_vocabulary_size_greater_than(de_words, n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaKh4U141kHV"
      },
      "source": [
        "## Analysing the sequence length (English and German)\n",
        "\n",
        "Here we compute the sequence length of the sequences in the English and German corpora. To ignore the outliers, we only consider data between the 1% and 99% quantiles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wc9dg-Rj1kHV",
        "outputId": "5b973c8a-8b78-4cf4-9da9-08be2f6af3db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English corpus\n",
            "==================================================\n",
            "\n",
            "Some summary statistics\n",
            "Median length: 6.0\n",
            "\n",
            "count    40000.000000\n",
            "mean         6.462525\n",
            "std          2.648050\n",
            "min          1.000000\n",
            "25%          5.000000\n",
            "50%          6.000000\n",
            "75%          8.000000\n",
            "max         37.000000\n",
            "Name: EN, dtype: float64\n",
            "\n",
            "Computing the statistics between the 1% and 99% quantiles (to ignore outliers)\n",
            "count    39462.000000\n",
            "mean         6.323400\n",
            "std          2.349188\n",
            "min          2.000000\n",
            "25%          5.000000\n",
            "50%          6.000000\n",
            "75%          8.000000\n",
            "max         14.000000\n",
            "Name: EN, dtype: float64\n",
            "\n",
            "German corpus\n",
            "==================================================\n",
            "\n",
            "Some summary statistics\n",
            "Median length: 8.0\n",
            "\n",
            "count    40000.000000\n",
            "mean         8.487650\n",
            "std          2.664052\n",
            "min          3.000000\n",
            "25%          7.000000\n",
            "50%          8.000000\n",
            "75%         10.000000\n",
            "max         46.000000\n",
            "Name: DE, dtype: float64\n",
            "\n",
            "Computing the statistics between the 1% and 99% quantiles (to ignore outliers)\n",
            "count    39131.000000\n",
            "mean         8.381309\n",
            "std          2.316330\n",
            "min          5.000000\n",
            "25%          7.000000\n",
            "50%          8.000000\n",
            "75%         10.000000\n",
            "max         16.000000\n",
            "Name: DE, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Section 11.1\n",
        "\n",
        "# Code listing 11.2\n",
        "def print_sequence_length(str_ser):\n",
        "\n",
        "    \"\"\" Print the summary stats of the sequence length \"\"\"\n",
        "\n",
        "    # Create a pd.Series, which contain the sequence length for each review\n",
        "    seq_length_ser = str_ser.str.split(' ').str.len()\n",
        "\n",
        "    # Get the median as well as summary statistics of the sequence length\n",
        "    print(\"\\nSome summary statistics\")\n",
        "    print(\"Median length: {}\\n\".format(seq_length_ser.median()))\n",
        "    print(seq_length_ser.describe())\n",
        "\n",
        "    # Get the quantiles at given marks\n",
        "    print(\"\\nComputing the statistics between the 1% and 99% quantiles (to ignore outliers)\")\n",
        "    p_01 = seq_length_ser.quantile(0.01)\n",
        "    p_99 = seq_length_ser.quantile(0.99)\n",
        "\n",
        "    # Print the summary stats of the data between the defined quantlies\n",
        "    print(seq_length_ser[(seq_length_ser >= p_01) & (seq_length_ser < p_99)].describe())\n",
        "\n",
        "print(\"English corpus\")\n",
        "print('='*50)\n",
        "print_sequence_length(train_df[\"EN\"])\n",
        "\n",
        "print(\"\\nGerman corpus\")\n",
        "print('='*50)\n",
        "print_sequence_length(train_df[\"DE\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oV7UgAIZ1kHW"
      },
      "source": [
        "## Printing the vocabulary size and sequence length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMru8ZuC1kHW",
        "outputId": "2d65bfb5-deeb-451b-85ae-82307fa3f0c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EN vocabulary size: 2249\n",
            "DE vocabulary size: 2531\n",
            "EN max sequence length: 19\n",
            "DE max sequence length: 21\n"
          ]
        }
      ],
      "source": [
        "print(\"EN vocabulary size: {}\".format(en_vocab))\n",
        "print(\"DE vocabulary size: {}\".format(de_vocab))\n",
        "\n",
        "# Define sequence lengths with some extra space for longer sequences\n",
        "en_seq_length = 19\n",
        "de_seq_length = 21\n",
        "\n",
        "print(\"EN max sequence length: {}\".format(en_seq_length))\n",
        "print(\"DE max sequence length: {}\".format(de_seq_length))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxT2yGy91kHW"
      },
      "source": [
        "## TensorFlow `TextVectorization` layer\n",
        "\n",
        "The `TextVectorization` layer takes in strings and convert them to token IDs. The layer can build a vocabulary using a given text corups and uses that to generate the token IDs."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.array(train_df[\"EN\"].tolist()).astype('str').shape,np.array(train_df[\"EN\"].tolist()).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRiS9l4rAD-I",
        "outputId": "33b3e5c2-563b-473a-fb0c-f8bf1686ff3a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((40000,), (40000,))"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uO_URXce1kHW",
        "outputId": "28e5fbb5-b283-4a53-8318-bc1472bb3f3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defined the vectorization layer for English\n",
            "Fitting the EN vectorization layer on data\n",
            "\tDone\n",
            "\n",
            "Defined the vectorization layer for German\n",
            "Fitting the DE vectorization layer on data\n",
            "\tDone\n"
          ]
        }
      ],
      "source": [
        "# Section 11.2\n",
        "\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "print(\"Defined the vectorization layer for English\")\n",
        "\n",
        "# Create the text vectorization layer (English)\n",
        "en_vectorize_layer = TextVectorization(\n",
        "    max_tokens=en_vocab,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=None\n",
        ")\n",
        "\n",
        "print(\"Fitting the EN vectorization layer on data\")\n",
        "# Here we are calling adapt to fit the vectorization layer with text\n",
        "# so that it learns the vocabulary\n",
        "en_vectorize_layer.adapt(np.array(train_df[\"EN\"].tolist()).astype('str'))\n",
        "print(\"\\tDone\")\n",
        "\n",
        "print(\"\\nDefined the vectorization layer for German\")\n",
        "\n",
        "# Create the text vectorization layer (German)\n",
        "de_vectorize_layer = TextVectorization(\n",
        "    max_tokens=de_vocab,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=de_seq_length,\n",
        "    pad_to_max_tokens=False,\n",
        ")\n",
        "\n",
        "print(\"Fitting the DE vectorization layer on data\")\n",
        "de_vectorize_layer.adapt(np.array(train_df[\"DE\"].tolist()))\n",
        "print(\"\\tDone\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxkHm0Xv1kHX"
      },
      "source": [
        "## `TextVectorization` layer in action\n",
        "\n",
        "### How to use the layer (EN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsZdsmpG1kHX",
        "outputId": "8c937cf1-6dbe-4b95-d7ac-af192ccee74c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 2s 2s/step\n",
            "Input data: \n",
            "[['run'], [\"I'll go home\"], ['ectoplasmic residue']]\n",
            "\n",
            "\n",
            "Token IDs: \n",
            "[[429   0   0]\n",
            " [ 82  43 111]\n",
            " [  1   1   0]]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow.keras.backend as K\n",
        "K.clear_session()\n",
        "\n",
        "# Create the model that uses the vectorize text layer\n",
        "toy_model = tf.keras.models.Sequential()\n",
        "\n",
        "# Start by creating an explicit input layer. It needs to have a shape of\n",
        "# (1,) (because we need to guarantee that there is exactly one string\n",
        "# input per batch), and the dtype needs to be 'string'.\n",
        "toy_model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
        "\n",
        "# The first layer in our model is the vectorization layer. After this\n",
        "# layer, we have a tensor of shape (batch_size, max_len) containing vocab\n",
        "# indices.\n",
        "toy_model.add(en_vectorize_layer)\n",
        "\n",
        "# Now, the model can map strings to integers,\n",
        "input_data = [[\"run\"], [\"I\\'ll go home\"],[\"ectoplasmic residue\"]]\n",
        "pred = toy_model.predict(input_data)\n",
        "\n",
        "print(\"Input data: \\n{}\\n\".format(input_data))\n",
        "print(\"\\nToken IDs: \\n{}\".format(pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7VWVrVO1kHY"
      },
      "source": [
        "### How to use the layer (DE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "044KYUmH1kHY",
        "outputId": "0d309cf3-8fd9-4264-c5f7-e3eef3e0b0d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 78ms/step\n",
            "Input data: \n",
            "[['[sos] Geh'], ['geh lauf']]\n",
            "\n",
            "\n",
            "Token IDs: \n",
            "[[  2 609   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0]\n",
            " [609   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0]]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow.keras.backend as K\n",
        "K.clear_session()\n",
        "\n",
        "# Create the model that uses the vectorize text layer\n",
        "toy_model = tf.keras.models.Sequential()\n",
        "\n",
        "# Start by creating an explicit input layer. It needs to have a shape of\n",
        "# (1,) (because we need to guarantee that there is exactly one string\n",
        "# input per batch), and the dtype needs to be 'string'.\n",
        "toy_model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
        "\n",
        "# The first layer in our model is the vectorization layer. After this\n",
        "# layer, we have a tensor of shape (batch_size, max_len) containing vocab\n",
        "# indices.\n",
        "toy_model.add(de_vectorize_layer)\n",
        "\n",
        "# Now, the model can map strings to integers,\n",
        "input_data = [[\"[sos] Geh\"], [\"geh lauf\"]]\n",
        "pred = toy_model.predict(input_data)\n",
        "\n",
        "print(\"Input data: \\n{}\\n\".format(input_data))\n",
        "print(\"\\nToken IDs: \\n{}\".format(pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91sYBv9x1kHY"
      },
      "source": [
        "### Sample of the vocabulary\n",
        "\n",
        "Let's print some words from the two vocabularies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WdOkobW1kHY",
        "outputId": "5bd53899-4079-4915-eff1-c6187e72ce6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English\n",
            "\n",
            "German\n",
            "None\n",
            "['', '[UNK]', 'sos', 'eos', 'ich', 'tom', 'nicht', 'ist', 'sie', 'du', 'das', 'zu', 'es', 'die', 'er', 'der', 'hat', 'dass', 'in', 'ein']\n",
            "2531\n"
          ]
        }
      ],
      "source": [
        "# Section 11.2\n",
        "\n",
        "print(\"English\")\n",
        "# Print first few words in the vocabulary\n",
        "# print(en_vectorize_layer.get_vocabulary()[:10])\n",
        "# # Print the size of the vocabulary\n",
        "# print(len(en_vectorize_layer.get_vocabulary()))\n",
        "\n",
        "print(\"\\nGerman\")\n",
        "# Print first few words in the vocabulary\n",
        "print(de_vectorize_layer._lookup_layer.input_vocabulary)\n",
        "print(de_vectorize_layer.get_vocabulary()[:20])\n",
        "\n",
        "# Print the size of the vocabulary\n",
        "print(len(de_vectorize_layer.get_vocabulary()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBAdIHan1kHY"
      },
      "source": [
        "## Defining the Seq2Seq model\n",
        "\n",
        "Here we define an encoder decoder model to translate between English and German. We will be using a bidirectional encoder and a standard decoder. The model will use Gated Recurrent Unit (GRU) as the recurrent component. The encoder and the decoder has their own `TextVectorization` layers as they use two different languages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "l7IWSMnE1kHY"
      },
      "outputs": [],
      "source": [
        "# Section 11.2\n",
        "\n",
        "import tensorflow.keras.backend as K\n",
        "K.clear_session()\n",
        "\n",
        "# Code listing 11.3\n",
        "def get_vectorizer(corpus, n_vocab, max_length=None, return_vocabulary=True, name=None):\n",
        "\n",
        "    \"\"\" Return a text vectorization layer or a model \"\"\"\n",
        "\n",
        "    # Definie an input layer that takes a list of strings (or an array of strings)\n",
        "    inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='encoder_input')\n",
        "\n",
        "    # When defining the vocab size, we'd add two for special tokens '' (Padding) and '[UNK]' (Oov tokens)\n",
        "    vectorize_layer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
        "        max_tokens=n_vocab+2,\n",
        "        output_mode='int',\n",
        "        output_sequence_length=max_length,\n",
        "    )\n",
        "\n",
        "    # Fit the vectorizer layer on the data\n",
        "    vectorize_layer.adapt(corpus)\n",
        "\n",
        "    # Get the token IDs\n",
        "    vectorized_out = vectorize_layer(inp)\n",
        "\n",
        "    if not return_vocabulary:\n",
        "        return tf.keras.models.Model(inputs=inp, outputs=vectorized_out, name=name)\n",
        "    else:\n",
        "        # Returns the vocabulary in addition to the model\n",
        "        return tf.keras.models.Model(inputs=inp, outputs=vectorized_out, name=name), vectorize_layer.get_vocabulary()\n",
        "\n",
        "# Code listing 11.4\n",
        "def get_encoder(n_vocab, vectorizer):\n",
        "    \"\"\" Define the encoder of the seq2seq model\"\"\"\n",
        "\n",
        "    # The input is (None,1) shaped and accepts an array of strings\n",
        "    inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='e_input')\n",
        "\n",
        "    # Vectorize the data (assign token IDs)\n",
        "    vectorized_out = vectorizer(inp)\n",
        "\n",
        "    # Define an embedding layer to convert IDs to word vectors\n",
        "    emb_layer = tf.keras.layers.Embedding(n_vocab+2, 128, mask_zero=True, name='e_embedding')\n",
        "    # Get the embeddings of the token IDs\n",
        "    emb_out = emb_layer(vectorized_out)\n",
        "\n",
        "    # Define a bidirectional GRU layer\n",
        "    # Encoder looks at the english text (i.e. the input) both backwards and forward\n",
        "    # this leads to better performance\n",
        "    gru_layer = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(128, name='e_gru'), name='e_bidirectional_gru')\n",
        "\n",
        "    # Get the output of the gru layer\n",
        "    gru_out = gru_layer(emb_out)\n",
        "\n",
        "    # Define the encoder model\n",
        "    encoder = tf.keras.models.Model(inputs=inp, outputs=gru_out, name='encoder')\n",
        "\n",
        "    return encoder\n",
        "\n",
        "\n",
        "# Code listing 11.5\n",
        "def get_final_seq2seq_model(n_vocab, encoder, vectorizer):\n",
        "    \"\"\" Define the final encoder-decoder model \"\"\"\n",
        "\n",
        "    # Encoder's input\n",
        "    e_inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='e_input_final')\n",
        "    # Get the encoders final output\n",
        "    d_init_state = encoder(e_inp)\n",
        "\n",
        "    # The input is (None,1) shaped and accepts an array of strings\n",
        "    # This input layer is used to train the seq2seq model with teacher-forcing\n",
        "    # we feed the German sequence as the input and ask the model to predict\n",
        "    # it with the words offset by 1 (i.e. next word)\n",
        "    d_inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='d_input')\n",
        "\n",
        "    # Vectorize the data (assign token IDs)\n",
        "    d_vectorized_out = vectorizer(d_inp)\n",
        "\n",
        "    # Define an embedding layer to convert IDs to word vectors\n",
        "    # Note that this is a different embedding layer to the encoder's embedding layer\n",
        "    d_emb_layer = tf.keras.layers.Embedding(n_vocab+2, 128, mask_zero=True, name='d_embedding')\n",
        "\n",
        "    # Get the embeddings of the token IDs\n",
        "    d_emb_out = d_emb_layer(d_vectorized_out)\n",
        "\n",
        "    # Define a GRU layer\n",
        "    # Unlike the encoder, we cannot define a bidirectional GRU for the decoder\n",
        "    # Why?\n",
        "    d_gru_layer = tf.keras.layers.GRU(256, return_sequences=True, name='d_gru')\n",
        "\n",
        "    # Get the output of the gru layer\n",
        "    d_gru_out = d_gru_layer(d_emb_out, initial_state=d_init_state)\n",
        "\n",
        "    # Define an intermediate dense layer\n",
        "    d_dense_layer_1 = tf.keras.layers.Dense(512, activation='relu', name='d_dense_1')\n",
        "    d_dense1_out = d_dense_layer_1(d_gru_out)\n",
        "\n",
        "    # The final prediction layer with softmax\n",
        "    d_dense_layer_final = tf.keras.layers.Dense(n_vocab+2, activation='softmax', name='d_dense_final')\n",
        "    d_final_out = d_dense_layer_final(d_dense1_out)\n",
        "\n",
        "    # Define the full model\n",
        "    seq2seq = tf.keras.models.Model(inputs=[e_inp, d_inp], outputs=d_final_out, name='final_seq2seq')\n",
        "\n",
        "    return seq2seq\n",
        "\n",
        "# Get the English vectorizer/vocabulary\n",
        "en_vectorizer, en_vocabulary = get_vectorizer(np.array(train_df[\"EN\"].tolist()), en_vocab, max_length=en_seq_length, name='e_vectorizer')\n",
        "# Get the German vectorizer/vocabulary\n",
        "de_vectorizer, de_vocabulary = get_vectorizer(np.array(train_df[\"DE\"].tolist()), de_vocab, max_length=de_seq_length-1, name='d_vectorizer')\n",
        "\n",
        "# Define the final model\n",
        "encoder = get_encoder(en_vocab, en_vectorizer)\n",
        "final_model = get_final_seq2seq_model(de_vocab, encoder, de_vectorizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "de_vectorizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lb4xXI_fId80",
        "outputId": "e8e718c7-09b0-4a76-97a2-5e3754d6e23b"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.engine.functional.Functional at 0x7e41df877c40>"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m08iNP2m1kHZ"
      },
      "source": [
        "## Compile the model\n",
        "\n",
        "Compile the model with a suitable loss, an optimizer and metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZsqaNMI1kHZ",
        "outputId": "9bcb0a7a-2e1b-4f42-aa47-2ee2de60b248"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"final_seq2seq\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " d_input (InputLayer)        [(None, 1)]                  0         []                            \n",
            "                                                                                                  \n",
            " d_vectorizer (Functional)   (None, 20)                   0         ['d_input[0][0]']             \n",
            "                                                                                                  \n",
            " e_input_final (InputLayer)  [(None, 1)]                  0         []                            \n",
            "                                                                                                  \n",
            " d_embedding (Embedding)     (None, 20, 128)              324224    ['d_vectorizer[0][0]']        \n",
            "                                                                                                  \n",
            " encoder (Functional)        (None, 256)                  486272    ['e_input_final[0][0]']       \n",
            "                                                                                                  \n",
            " d_gru (GRU)                 (None, 20, 256)              296448    ['d_embedding[0][0]',         \n",
            "                                                                     'encoder[0][0]']             \n",
            "                                                                                                  \n",
            " d_dense_1 (Dense)           (None, 20, 512)              131584    ['d_gru[0][0]']               \n",
            "                                                                                                  \n",
            " d_dense_final (Dense)       (None, 20, 2533)             1299429   ['d_dense_1[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2537957 (9.68 MB)\n",
            "Trainable params: 2537957 (9.68 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Section 11.2\n",
        "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
        "\n",
        "# Compile the model\n",
        "final_model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "final_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXPlazbL1kHZ"
      },
      "source": [
        "## Evaluating MT models - BLEU metric\n",
        "\n",
        "In machine translation, a popular choice for assessing performance is the BiLingual Evaluation Understudy (BLEU) metric. Word-to-word accuracy does not reflect the true performance of these models as there can be different ways the same phrase can be translated to. BLEU can take into account such multiple translations when computing the final score. Furthermore, BLEU is superior because it measures precision at multiple n-gram scales between the actual and predicted translations.\n",
        "\n",
        "The implementation is inspired by: https://github.com/tensorflow/nmt/blob/master/nmt/scripts/bleu.py\n",
        "\n",
        "### Defining the BLEU metric\n",
        "\n",
        "Below we define a `BLEUMetric` object that can be used to compute the performance of the model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sacrebleu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vCN2KgGbXu0",
        "outputId": "8fdca6fc-e66f-44e2-caf8-27e6466a86ab"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2023.6.3)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (1.23.5)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.3)\n",
            "Installing collected packages: portalocker, colorama, sacrebleu\n",
            "Successfully installed colorama-0.4.6 portalocker-2.8.2 sacrebleu-2.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright 2017 Google Inc. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\n",
        "\"\"\"Python implementation of BLEU and smooth-BLEU.\n",
        "\n",
        "This module provides a Python implementation of BLEU and smooth-BLEU.\n",
        "Smooth BLEU is computed following the method outlined in the paper:\n",
        "Chin-Yew Lin, Franz Josef Och. ORANGE: a method for evaluating automatic\n",
        "evaluation metrics for machine translation. COLING 2004.\n",
        "\"\"\"\n",
        "\n",
        "import collections\n",
        "import math\n",
        "\n",
        "\n",
        "def _get_ngrams(segment, max_order):\n",
        "  \"\"\"Extracts all n-grams upto a given maximum order from an input segment.\n",
        "\n",
        "  Args:\n",
        "    segment: text segment from which n-grams will be extracted.\n",
        "    max_order: maximum length in tokens of the n-grams returned by this\n",
        "        methods.\n",
        "\n",
        "  Returns:\n",
        "    The Counter containing all n-grams upto max_order in segment\n",
        "    with a count of how many times each n-gram occurred.\n",
        "  \"\"\"\n",
        "  ngram_counts = collections.Counter()\n",
        "  for order in range(1, max_order + 1):\n",
        "    for i in range(0, len(segment) - order + 1):\n",
        "      ngram = tuple(segment[i:i+order])\n",
        "      ngram_counts[ngram] += 1\n",
        "  return ngram_counts\n",
        "\n",
        "\n",
        "def compute_bleu(reference_corpus, translation_corpus, max_order=4,\n",
        "                 smooth=False):\n",
        "  \"\"\"Computes BLEU score of translated segments against one or more references.\n",
        "\n",
        "  Args:\n",
        "    reference_corpus: list of lists of references for each translation. Each\n",
        "        reference should be tokenized into a list of tokens.\n",
        "    translation_corpus: list of translations to score. Each translation\n",
        "        should be tokenized into a list of tokens.\n",
        "    max_order: Maximum n-gram order to use when computing BLEU score.\n",
        "    smooth: Whether or not to apply Lin et al. 2004 smoothing.\n",
        "\n",
        "  Returns:\n",
        "    3-Tuple with the BLEU score, n-gram precisions, geometric mean of n-gram\n",
        "    precisions and brevity penalty.\n",
        "  \"\"\"\n",
        "  matches_by_order = [0] * max_order\n",
        "  possible_matches_by_order = [0] * max_order\n",
        "  reference_length = 0\n",
        "  translation_length = 0\n",
        "  for (references, translation) in zip(reference_corpus,\n",
        "                                       translation_corpus):\n",
        "    reference_length += min(len(r) for r in references)\n",
        "    translation_length += len(translation)\n",
        "\n",
        "    merged_ref_ngram_counts = collections.Counter()\n",
        "    for reference in references:\n",
        "      merged_ref_ngram_counts |= _get_ngrams(reference, max_order)\n",
        "    translation_ngram_counts = _get_ngrams(translation, max_order)\n",
        "    overlap = translation_ngram_counts & merged_ref_ngram_counts\n",
        "    for ngram in overlap:\n",
        "      matches_by_order[len(ngram)-1] += overlap[ngram]\n",
        "    for order in range(1, max_order+1):\n",
        "      possible_matches = len(translation) - order + 1\n",
        "      if possible_matches > 0:\n",
        "        possible_matches_by_order[order-1] += possible_matches\n",
        "\n",
        "  precisions = [0] * max_order\n",
        "  for i in range(0, max_order):\n",
        "    if smooth:\n",
        "      precisions[i] = ((matches_by_order[i] + 1.) /\n",
        "                       (possible_matches_by_order[i] + 1.))\n",
        "    else:\n",
        "      if possible_matches_by_order[i] > 0:\n",
        "        precisions[i] = (float(matches_by_order[i]) /\n",
        "                         possible_matches_by_order[i])\n",
        "      else:\n",
        "        precisions[i] = 0.0\n",
        "\n",
        "  if min(precisions) > 0:\n",
        "    p_log_sum = sum((1. / max_order) * math.log(p) for p in precisions)\n",
        "    geo_mean = math.exp(p_log_sum)\n",
        "  else:\n",
        "    geo_mean = 0\n",
        "\n",
        "  ratio = float(translation_length) / reference_length\n",
        "\n",
        "  if ratio > 1.0:\n",
        "    bp = 1.\n",
        "  else:\n",
        "    bp = math.exp(1 - 1. / ratio)\n",
        "\n",
        "  bleu = geo_mean * bp\n",
        "\n",
        "  return (bleu, precisions, bp, ratio, translation_length, reference_length)"
      ],
      "metadata": {
        "id": "3-ZSUwkubZ6Z"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "id": "mYRv90f41kHZ"
      },
      "outputs": [],
      "source": [
        "# Section 11.3\n",
        "\n",
        "from tensorflow.keras.layers.experimental.preprocessing import StringLookup\n",
        "# from bleu import compute_bleu\n",
        "\n",
        "# Code listing 11.8\n",
        "class BLEUMetric(object):\n",
        "\n",
        "    def __init__(self, vocabulary, name='perplexity', **kwargs):\n",
        "      \"\"\" Computes the BLEU score (Metric for machine translation) \"\"\"\n",
        "      super().__init__()\n",
        "      self.vocab = vocabulary\n",
        "      self.id_to_token_layer = StringLookup(vocabulary=self.vocab, num_oov_indices=0, oov_token=\"[KNU]\", invert=True)\n",
        "\n",
        "    def calculate_bleu_from_predictions(self, real, pred):\n",
        "        \"\"\" Calculate the BLEU score for targets and predictions \"\"\"\n",
        "\n",
        "        # Get the predicted token IDs\n",
        "        pred_argmax = tf.argmax(pred, axis=-1)\n",
        "\n",
        "        # Convert token IDs to words using the vocabulary and the StringLookup\n",
        "        pred_tokens = self.id_to_token_layer(pred_argmax)\n",
        "        real_tokens = self.id_to_token_layer(real)\n",
        "\n",
        "        def clean_text(tokens):\n",
        "\n",
        "            \"\"\" Clean padding and [SOS]/[EOS] tokens to only keep meaningful words \"\"\"\n",
        "\n",
        "            # 3. Strip the string of any extra white spaces\n",
        "            translations_in_bytes = tf.strings.strip(\n",
        "                        # 2. Replace everything after the eos token with blank\n",
        "                        tf.strings.regex_replace(\n",
        "                            # 1. Join all the tokens to one string in each sequence\n",
        "                            tf.strings.join(\n",
        "                                tf.transpose(tokens), separator=' '\n",
        "                            ),\n",
        "                        \"eos.*\", \"\"),\n",
        "                   )\n",
        "\n",
        "            # Decode the byte stream to a string\n",
        "            translations = np.char.decode(\n",
        "                translations_in_bytes.numpy().astype(np.bytes_), encoding='utf-8'\n",
        "            )\n",
        "\n",
        "            # If the string is empty, add a [UNK] token\n",
        "            # Otherwise get a Division by zero error\n",
        "            translations = [sent if len(sent)>0 else '[UNK]' for sent in translations ]\n",
        "\n",
        "            # Split the sequences to individual tokens\n",
        "            translations = np.char.split(translations).tolist()\n",
        "\n",
        "            return translations\n",
        "\n",
        "        # Get the clean versions of the predictions and real seuqences\n",
        "        pred_tokens = clean_text(pred_tokens)\n",
        "        # We have to wrap each real sequence in a list to make use of a function to compute bleu\n",
        "        real_tokens = [[token_seq] for token_seq in clean_text(real_tokens)]\n",
        "\n",
        "        # The compute_bleu method accpets the translations and references in the following format\n",
        "        # tranlation - list of list of tokens\n",
        "        # references - list of list of list of tokens\n",
        "        bleu, precisions, bp, ratio, translation_length, reference_length = compute_bleu(real_tokens, pred_tokens, smooth=False)\n",
        "\n",
        "        return bleu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRjRwj6Y1kHZ"
      },
      "source": [
        "### Using the BLEU metric\n",
        "\n",
        "Below you can see BLEU being used to computer the similarity between a translation (predicted) and reference (true target)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRvHCFau1kHa",
        "outputId": "c73dbcb3-bf3d-416f-b439-aaa5e34ae617"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU score with longer correctly predicte phrases: 0.7598356856515925\n",
            "BLEU score without longer correctly predicte phrases: 0.537284965911771\n"
          ]
        }
      ],
      "source": [
        "translation = [['[UNK]', '[UNK]', 'mÃssen', 'wir', 'in', 'erfahrung', 'bringen', 'wo', 'sie', 'wohnen']]\n",
        "reference = [[['als', 'mÃssen', 'mÃssen', 'wir', 'in', 'erfahrung', 'bringen', 'wo', 'sie', 'wohnen']]]\n",
        "bleu1, precisions, bp, ratio, translation_length, reference_length = compute_bleu( reference,translation)\n",
        "\n",
        "translation = [['[UNK]', 'einmal', 'mÃssen', '[UNK]', 'in', 'erfahrung', 'bringen', 'wo', 'sie', 'wohnen']]\n",
        "reference = [[['als', 'mÃssen', 'mÃssen', 'wir', 'in', 'erfahrung', 'bringen', 'wo', 'sie', 'wohnen']]]\n",
        "\n",
        "\n",
        "bleu2, precisions, bp, ratio, translation_length, reference_length = compute_bleu(reference,translation)\n",
        "\n",
        "print(\"BLEU score with longer correctly predicte phrases: {}\".format(bleu1))\n",
        "print(\"BLEU score without longer correctly predicte phrases: {}\".format(bleu2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7hb3-xO1kHa"
      },
      "source": [
        "## Training the model with a custom loop\n",
        "\n",
        "We will train the model using a custom loop as we want to incorporate BLEU as a metric in our training. We will follow the following procedure;\n",
        "\n",
        "* Each epoch,\n",
        "  * Shuffle the training data\n",
        "  * Train our model on all the training data (in batches)\n",
        "  * Evaluate the model on validation data\n",
        "* Finally, evaluate the model on test data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Sample DataFrame\n",
        "data = {'DE': [\"This is the first example.\", \"Second example here.\", \"Another one\"]}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Apply the code to the \"DE\" column\n",
        "result = np.array(df[\"DE\"].str.rsplit(n=1, expand=True).iloc[:, 0].tolist())\n",
        "result2 = np.array(df[\"DE\"].str.rsplit(n=1, expand=True))\n",
        "print(result2)\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0g7WnuFsUr1S",
        "outputId": "9754559a-fb06-4ed8-9212-403931a75c16"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['This is the first' 'example.']\n",
            " ['Second example' 'here.']\n",
            " ['Another' 'one']]\n",
            "['This is the first' 'Second example' 'Another']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shuffle_data?"
      ],
      "metadata": {
        "id": "uUF00KoNaEvt"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {
        "id": "eZHdlS291kHa"
      },
      "outputs": [],
      "source": [
        "# Section 11.3\n",
        "import time\n",
        "\n",
        "epochs = 20\n",
        "batch_size = 128\n",
        "\n",
        "# Code listing 11.6\n",
        "def prepare_data(train_df, valid_df, test_df):\n",
        "    \"\"\" Create a data dictionary from the dataframes containing data \"\"\"\n",
        "\n",
        "    data_dict = {}\n",
        "    for label, df in zip(['train', 'valid', 'test'], [train_df, valid_df, test_df]):\n",
        "        en_inputs = np.array(df[\"EN\"].tolist())\n",
        "        de_inputs = np.array(df[\"DE\"].str.rsplit(n=1, expand=True).iloc[:,0].tolist())\n",
        "        de_labels = np.array(df[\"DE\"].str.split(n=1, expand=True).iloc[:,1].tolist())\n",
        "        data_dict[label] = {'encoder_inputs': en_inputs, 'decoder_inputs': de_inputs, 'decoder_labels': de_labels}\n",
        "\n",
        "    return data_dict\n",
        "\n",
        "# Code listing 11.7\n",
        "def shuffle_data(en_inputs, de_inputs, de_labels, shuffle_inds=None):\n",
        "    \"\"\" Shuffle the data randomly (but all of inputs and labels at ones)\"\"\"\n",
        "\n",
        "    if shuffle_inds is None:\n",
        "        # If shuffle_inds are not passed create a shuffling automatically\n",
        "        shuffle_inds = np.random.permutation(np.arange(en_inputs.shape[0]))\n",
        "    else:\n",
        "        # Shuffle the provided shuffle_inds\n",
        "        shuffle_inds = np.random.permutation(shuffle_inds)\n",
        "\n",
        "    # Return shuffled data\n",
        "    return (en_inputs[shuffle_inds], de_inputs[shuffle_inds], de_labels[shuffle_inds]), shuffle_inds\n",
        "\n",
        "\n",
        "# Code listing 11.9\n",
        "def evaluate_model(model, vectorizer, en_inputs_raw, de_inputs_raw, de_labels_raw, batch_size):\n",
        "    \"\"\" Evaluate the model on various metrics such as loss, accuracy and BLEU \"\"\"\n",
        "\n",
        "    # Define the metric\n",
        "    bleu_metric = BLEUMetric(de_vocabulary)\n",
        "\n",
        "    loss_log, accuracy_log, bleu_log = [], [], []\n",
        "    # Get the number of batches\n",
        "    n_batches = en_inputs_raw.shape[0]//batch_size\n",
        "    print(\" \", end='\\r')\n",
        "\n",
        "    # Evaluate one batch at a time\n",
        "    for i in range(n_batches):\n",
        "        # Status update\n",
        "        print(\"Evaluating batch {}/{}\".format(i+1, n_batches), end='\\r')\n",
        "\n",
        "        # Get the inputs and targers\n",
        "        x = [en_inputs_raw[i*batch_size:(i+1)*batch_size], de_inputs_raw[i*batch_size:(i+1)*batch_size]]\n",
        "        y = vectorizer(de_labels_raw[i*batch_size:(i+1)*batch_size])\n",
        "\n",
        "        # Get the evaluation metrics\n",
        "        loss, accuracy = model.evaluate(x, y, verbose=0)\n",
        "        # Get the predictions to compute BLEU\n",
        "        pred_y = model.predict(x, verbose=0)\n",
        "\n",
        "        # Update logs\n",
        "        loss_log.append(loss)\n",
        "        accuracy_log.append(accuracy)\n",
        "        bleu_log.append(bleu_metric.calculate_bleu_from_predictions(y, pred_y))\n",
        "\n",
        "    return np.mean(loss_log), np.mean(accuracy_log), np.mean(bleu_log)\n",
        "\n",
        "\n",
        "# Code listing 11.10\n",
        "def train_model(model, vectorizer, train_df, valid_df, test_df, epochs, batch_size):\n",
        "    \"\"\" Training the model and evaluating on validation/test sets \"\"\"\n",
        "\n",
        "    # Define the metric\n",
        "    bleu_metric = BLEUMetric(de_vocabulary)\n",
        "\n",
        "    # Define the data\n",
        "    data_dict = prepare_data(train_df, valid_df, test_df)\n",
        "\n",
        "    shuffle_inds = None\n",
        "\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        # Reset metric logs every epoch\n",
        "        bleu_log = []\n",
        "        accuracy_log = []\n",
        "        loss_log = []\n",
        "\n",
        "        # =================================================================== #\n",
        "        #                         Train Phase                                 #\n",
        "        # =================================================================== #\n",
        "\n",
        "        # Shuffle data at the beginning of every epoch\n",
        "        (en_inputs_raw,de_inputs_raw,de_labels_raw), shuffle_inds  = shuffle_data(\n",
        "            data_dict['train']['encoder_inputs'],\n",
        "            data_dict['train']['decoder_inputs'],\n",
        "            data_dict['train']['decoder_labels'],\n",
        "            shuffle_inds\n",
        "        )\n",
        "\n",
        "        # Get the number of training batches\n",
        "        n_train_batches = en_inputs_raw.shape[0]//batch_size\n",
        "\n",
        "        # Train one batch at a time\n",
        "        for i in range(n_train_batches):\n",
        "            # Status update\n",
        "            print(\"Training batch {}/{}\".format(i+1, n_train_batches), end='\\r')\n",
        "\n",
        "            # Get a batch of inputs (english and german sequences)\n",
        "            x = [en_inputs_raw[i*batch_size:(i+1)*batch_size], de_inputs_raw[i*batch_size:(i+1)*batch_size]]\n",
        "            # Get a batch of targets (german sequences offset by 1)\n",
        "            y = vectorizer(de_labels_raw[i*batch_size:(i+1)*batch_size])\n",
        "\n",
        "            # Train for a single step\n",
        "            model.train_on_batch(x, y)\n",
        "            # Evaluate the model to get the metrics\n",
        "            loss, accuracy = model.evaluate(x, y, verbose=0)\n",
        "            # Get the final prediction to compute BLEU\n",
        "            pred_y = model.predict(x, verbose=0)\n",
        "\n",
        "            # Update the epoch's log records of the metrics\n",
        "            loss_log.append(loss)\n",
        "            accuracy_log.append(accuracy)\n",
        "            bleu_log.append(bleu_metric.calculate_bleu_from_predictions(y, pred_y))\n",
        "\n",
        "        # =================================================================== #\n",
        "        #                      Validation Phase                               #\n",
        "        # =================================================================== #\n",
        "\n",
        "        val_en_inputs = data_dict['valid']['encoder_inputs']\n",
        "        val_de_inputs = data_dict['valid']['decoder_inputs']\n",
        "        val_de_labels = data_dict['valid']['decoder_labels']\n",
        "\n",
        "\n",
        "        val_loss, val_accuracy, val_bleu = evaluate_model(\n",
        "            model, vectorizer, val_en_inputs, val_de_inputs, val_de_labels, batch_size\n",
        "        )\n",
        "\n",
        "        # Print the evaluation metrics of each epoch\n",
        "        print(\"\\nEpoch {}/{}\".format(epoch+1, epochs))\n",
        "        print(\"\\t(train) loss: {} - accuracy: {} - bleu: {}\".format(np.mean(loss_log), np.mean(accuracy_log), np.mean(bleu_log)))\n",
        "        print(\"\\t(valid) loss: {} - accuracy: {} - bleu: {}\".format(val_loss, val_accuracy, val_bleu))\n",
        "\n",
        "    # =================================================================== #\n",
        "    #                      Test Phase                                     #\n",
        "    # =================================================================== #\n",
        "\n",
        "    test_en_inputs = data_dict['test']['encoder_inputs']\n",
        "    test_de_inputs = data_dict['test']['decoder_inputs']\n",
        "    test_de_labels = data_dict['test']['decoder_labels']\n",
        "\n",
        "    test_loss, test_accuracy, test_bleu = evaluate_model(\n",
        "            model, vectorizer, test_en_inputs, test_de_inputs, test_de_labels, batch_size\n",
        "    )\n",
        "\n",
        "    print(\"\\n(test) loss: {} - accuracy: {} - bleu: {}\".format(test_loss, test_accuracy, test_bleu))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hhojl_6j1kHb",
        "outputId": "fb578d4e-5a0e-4d27-ef2e-91073d184e6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/numeric.py:2463: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  return bool(asarray(a1 == a2).all())\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/20\n",
            "\t(train) loss: 1.9199708841550045 - accuracy: 0.5684154780629354 - bleu: 0.17769361816542759\n",
            "\t(valid) loss: 2.3556002653562107 - accuracy: 0.5229704678058624 - bleu: 0.14201647989539168\n",
            "\n",
            "Epoch 2/20\n",
            "\t(train) loss: 1.7022911310195923 - accuracy: 0.6046673903862635 - bleu: 0.21492502040509373\n",
            "\t(valid) loss: 2.2823529885365414 - accuracy: 0.5370764678869492 - bleu: 0.14458658618475062\n",
            "\n",
            "Epoch 3/20\n",
            "\t(train) loss: 1.5186454474161832 - accuracy: 0.6375646174718173 - bleu: 0.2522937233383355\n",
            "\t(valid) loss: 2.237964630126953 - accuracy: 0.548319147183345 - bleu: 0.15983428624131865\n",
            "\n",
            "Epoch 4/20\n",
            "\t(train) loss: 1.3600360602140427 - accuracy: 0.6670013371950541 - bleu: 0.2875413274894916\n",
            "\t(valid) loss: 2.2323324221831102 - accuracy: 0.5552270336028857 - bleu: 0.17212886738521835\n",
            "\n",
            "Epoch 5/20\n",
            "\t(train) loss: 1.2205789177081523 - accuracy: 0.6947064195305873 - bleu: 0.32374389556734884\n",
            "\t(valid) loss: 2.2501599177336082 - accuracy: 0.5569638380637536 - bleu: 0.16921888136982924\n",
            "\n",
            "Epoch 6/20\n",
            "\t(train) loss: 1.0969359989349659 - accuracy: 0.7217567341449933 - bleu: 0.3608438070782417\n",
            "\t(valid) loss: 2.28499921468588 - accuracy: 0.5618284696187729 - bleu: 0.18008674742062977\n",
            "\n",
            "Epoch 7/20\n",
            "\t(train) loss: 0.9856789717689539 - accuracy: 0.7471483493080506 - bleu: 0.40162079841747134\n",
            "\t(valid) loss: 2.332485223427797 - accuracy: 0.5627758686359112 - bleu: 0.17366162421209008\n",
            "\n",
            "Epoch 8/20\n",
            "\t(train) loss: 0.8860765515993803 - accuracy: 0.7698705448554113 - bleu: 0.43962983207123496\n",
            "\t(valid) loss: 2.38185047186338 - accuracy: 0.563351530295152 - bleu: 0.17833003760152435\n",
            "\n",
            "Epoch 9/20\n",
            "\t(train) loss: 0.7962949967537171 - accuracy: 0.7921207524262942 - bleu: 0.47717179234979584\n",
            "\t(valid) loss: 2.4563517631628575 - accuracy: 0.5615357833030896 - bleu: 0.1777555762801009\n",
            "\n",
            "Epoch 10/20\n",
            "\t(train) loss: 0.7141067456358519 - accuracy: 0.8126014484426914 - bleu: 0.517178613057867\n",
            "\t(valid) loss: 2.526084753183218 - accuracy: 0.5607067743937174 - bleu: 0.17416459381748287\n",
            "\n",
            "Epoch 11/20\n",
            "\t(train) loss: 0.6420175130359638 - accuracy: 0.8312047244264529 - bleu: 0.5551259066083597\n",
            "\t(valid) loss: 2.624644010494917 - accuracy: 0.5581789872585199 - bleu: 0.17410935665417485\n",
            "\n",
            "Epoch 12/20\n",
            "\t(train) loss: 0.574842105500209 - accuracy: 0.8489266527004731 - bleu: 0.5920995690236075\n",
            "\t(valid) loss: 2.7317982025635548 - accuracy: 0.5519495835671058 - bleu: 0.16355264070828618\n",
            "\n",
            "Epoch 13/20\n",
            "\t(train) loss: 0.515089429055269 - accuracy: 0.8650870609741944 - bleu: 0.6295218312926334\n",
            "\t(valid) loss: 2.8303811183342567 - accuracy: 0.5520072212586036 - bleu: 0.1606620508089027\n",
            "\n",
            "Epoch 14/20\n",
            "\t(train) loss: 0.46302028105427057 - accuracy: 0.8789169421562781 - bleu: 0.6610047132786909\n",
            "\t(valid) loss: 2.954386148697291 - accuracy: 0.5512283979318081 - bleu: 0.16915962363324516\n",
            "\n",
            "Epoch 15/20\n",
            "\t(train) loss: 0.41256685258868414 - accuracy: 0.8925960719203337 - bleu: 0.6955774234189044\n",
            "\t(valid) loss: 3.046806989571987 - accuracy: 0.5503231011904203 - bleu: 0.16877559739647738\n",
            "\n",
            "Epoch 16/20\n",
            "\t(train) loss: 0.36953321280769813 - accuracy: 0.9048961656980026 - bleu: 0.7280963980273322\n",
            "\t(valid) loss: 3.1488049702766614 - accuracy: 0.5477368770501553 - bleu: 0.16341891036157888\n",
            "\n",
            "Epoch 17/20\n",
            "\t(train) loss: 0.3307847935610857 - accuracy: 0.9156483583725415 - bleu: 0.755951858113123\n",
            "\t(valid) loss: 3.272658476462731 - accuracy: 0.5450906875805978 - bleu: 0.1653847429328091\n",
            "\n",
            "Epoch 18/20\n",
            "\t(train) loss: 0.2921692748577931 - accuracy: 0.9259739210590338 - bleu: 0.786982108219628\n",
            "\t(valid) loss: 3.4112500655345426 - accuracy: 0.5464951380705222 - bleu: 0.1656038829858435\n",
            "\n",
            "Epoch 19/20\n",
            "\t(train) loss: 0.2632228973775338 - accuracy: 0.935045733092687 - bleu: 0.8105156414592667\n",
            "\t(valid) loss: 3.5039922335208993 - accuracy: 0.5461650918691586 - bleu: 0.16309436077200737\n",
            "\n",
            "Epoch 20/20\n",
            "\t(train) loss: 0.23550952011002943 - accuracy: 0.9421586459263777 - bleu: 0.8312663179477187\n",
            "\t(valid) loss: 3.61238187398666 - accuracy: 0.5445928130394373 - bleu: 0.1659797363732199\n",
            "\n",
            "(test) loss: 3.6168954433538976 - accuracy: 0.5454466121319013 - bleu: 0.15931075659508845\n",
            "\n",
            "It took 2202.6449780464172 seconds to complete the training\n"
          ]
        }
      ],
      "source": [
        "\n",
        "t1 = time.time()\n",
        "train_model(final_model, de_vectorizer, train_df, valid_df, test_df, epochs, batch_size)\n",
        "t2 = time.time()\n",
        "\n",
        "print(\"\\nIt took {} seconds to complete the training\".format(t2-t1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REtdm-Re1kHb"
      },
      "source": [
        "## Save the trained model\n",
        "\n",
        "We save the trained model as well as the vocabularies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "id": "kODII8Ps1kHb"
      },
      "outputs": [],
      "source": [
        "# Section 11.3\n",
        "\n",
        "## Save the model\n",
        "os.makedirs('models', exist_ok=True)\n",
        "tf.keras.models.save_model(final_model, os.path.join('models', 'seq2seq'))\n",
        "\n",
        "import json\n",
        "os.makedirs(os.path.join('models', 'seq2seq_vocab'), exist_ok=True)\n",
        "\n",
        "# Save the vocabulary files\n",
        "with open(os.path.join('models', 'seq2seq_vocab', 'en_vocab.json'), 'w') as f:\n",
        "    json.dump(en_vocabulary, f)\n",
        "with open(os.path.join('models', 'seq2seq_vocab', 'de_vocab.json'), 'w') as f:\n",
        "    json.dump(de_vocabulary, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cx1c8pYu1kHb"
      },
      "source": [
        "## Defining the inference model\n",
        "\n",
        "For inference we have to create a new model using the weights of the trained model. During training we used teacher forcing, i.e. providing words from the translation as inputs to the decoder. This cannot be done during inference as we do not have a translation, but want to generate one.\n",
        "\n",
        "Therefore, we create a decoder model that can generate one prediction at a time. We start the prediction process by giving the `sos` token as the initial input to the decoder and keep generating words until the decoder outputs `eos`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Q11gRgY1kHb",
        "outputId": "96553fce-ba7d-4714-953b-84dd1dc0b1a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading vocabularies\n",
            "Loading weights and generating the inference model\n",
            "\tDone\n"
          ]
        }
      ],
      "source": [
        "# Section 11.4\n",
        "\n",
        "# Code listing 11.11\n",
        "import tensorflow.keras.backend as K\n",
        "K.clear_session()\n",
        "\n",
        "def get_inference_model(save_path):\n",
        "    \"\"\" Load the saved model and create an inference model from that \"\"\"\n",
        "\n",
        "    # Load the model\n",
        "    model = tf.keras.models.load_model(save_path)\n",
        "\n",
        "    # Get the encoder model\n",
        "    en_model = model.get_layer(\"encoder\")\n",
        "\n",
        "    # Define two inputs\n",
        "    # 1. Takes a single word as the input to the decoder\n",
        "    d_inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='d_infer_input')\n",
        "    # 2. Takes an initial state to pass to the decoder GRU as an input\n",
        "    d_state_inp = tf.keras.Input(shape=(256,), name='d_infer_state')\n",
        "\n",
        "    # Generate the vectorized output of inp\n",
        "    d_vectorizer = model.get_layer('d_vectorizer')\n",
        "    d_vectorized_out = d_vectorizer(d_inp)\n",
        "\n",
        "    # Generate the embeddings from the vectorized input\n",
        "    d_emb_out = model.get_layer('d_embedding')(d_vectorized_out)\n",
        "\n",
        "    # Get the GRU layer\n",
        "    d_gru_layer = model.get_layer(\"d_gru\")\n",
        "    # Since we generate one word at a time, we will not need the return_sequences\n",
        "    d_gru_layer.return_sequences = False\n",
        "    # Get the GRU out while using d_state_inp from earlier, as the initial state\n",
        "    d_gru_out = d_gru_layer(d_emb_out, initial_state=d_state_inp)\n",
        "\n",
        "    # Get the dense output\n",
        "    d_dense1_out = model.get_layer(\"d_dense_1\")(d_gru_out)\n",
        "\n",
        "    # Get the final output\n",
        "    d_final_out = model.get_layer(\"d_dense_final\")(d_dense1_out)\n",
        "\n",
        "    # Define the final decoder\n",
        "    de_model = tf.keras.models.Model(inputs=[d_inp, d_state_inp], outputs=[d_final_out, d_gru_out])\n",
        "\n",
        "    return en_model, de_model\n",
        "\n",
        "def get_vocabularies(save_dir):\n",
        "    \"\"\" Load the vocabulary files from a given path\"\"\"\n",
        "\n",
        "    with open(os.path.join(save_dir, 'en_vocab.json'), 'r') as f:\n",
        "        en_vocabulary = json.load(f)\n",
        "\n",
        "    with open(os.path.join(save_dir, 'de_vocab.json'), 'r') as f:\n",
        "        de_vocabulary = json.load(f)\n",
        "\n",
        "    return en_vocabulary, de_vocabulary\n",
        "\n",
        "print(\"Loading vocabularies\")\n",
        "en_vocabulary, de_vocabulary = get_vocabularies(os.path.join('models', 'seq2seq_vocab'))\n",
        "\n",
        "print(\"Loading weights and generating the inference model\")\n",
        "en_model, de_model = get_inference_model(os.path.join('models', 'seq2seq'))\n",
        "print(\"\\tDone\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "579lN5yo1kHc"
      },
      "source": [
        "## Generating new translations\n",
        "\n",
        "Here we generate a new translation by first starting with the `sos` token and asking the decoder to generate words until it outputs `eos`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsNC6wl_1kHc",
        "outputId": "466cfdd7-0533-4ae7-c770-7b301aca3ef8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: Tom has been married three times.\n",
            "Translation: tom ist schon seit drei tagen eos\n",
            "\n",
            "Input: Aren't you going with us?\n",
            "Translation: [UNK] sie uns nicht eos\n",
            "\n",
            "Input: Tom and Mary looked at each other and then back at John.\n",
            "Translation: tom und maria sahen einander an und lächelte sich an zu küssen eos\n",
            "\n",
            "Input: My grandmother likes to weave things.\n",
            "Translation: meine großmutter [UNK] gerne musik eos\n",
            "\n",
            "Input: I cannot afford to buy a new car.\n",
            "Translation: ich kann es mir kein neues auto leisten sollen eos\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Code listing 11.12\n",
        "def generate_new_translation(en_model, de_model, de_vocabulary, sample_en_text):\n",
        "    \"\"\" Generate a new translation \"\"\"\n",
        "\n",
        "    start_token = 'sos'\n",
        "\n",
        "    # Print the input\n",
        "    print(\"Input: {}\".format(sample_en_text))\n",
        "\n",
        "    # Get the initial state for the decoder\n",
        "    d_state = en_model.predict(np.array([sample_en_text]), verbose=0)\n",
        "    # First word will be sos\n",
        "    de_word = start_token\n",
        "    # We collect the translation in this list\n",
        "    de_translation = []\n",
        "\n",
        "    # Keep predicting until we get eos\n",
        "    while de_word != 'eos':\n",
        "        # Override the previous state input with the new state\n",
        "        de_pred, d_state = de_model.predict([np.array([de_word]), d_state], verbose=0)\n",
        "        # Get the actual word from the token ID of the prediction\n",
        "        de_word = de_vocabulary[np.argmax(de_pred)]\n",
        "        # Add that to the translation\n",
        "        de_translation.append(de_word)\n",
        "\n",
        "    print(\"Translation: {}\\n\".format(' '.join(de_translation)))\n",
        "\n",
        "for i in range(5):\n",
        "    sample_en_text = test_df[\"EN\"].iloc[i]\n",
        "    generate_new_translation(en_model, de_model, de_vocabulary, sample_en_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjp_m8Ky1kHc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}